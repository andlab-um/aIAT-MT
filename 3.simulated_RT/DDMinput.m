%%
ques=xlsread('/Users/orlacamus/Desktop/projects/IAT240615/questionnaire_IAT/ques.xlsx');
PN=rescale(ques(:,11))*0.5+0.8;
%%

Agents  = 58;
N       = 4;                            % Total number of neurons (ev1 / ev2 / true / false)
image   = [1 3];                        % Neurons encoding self-image (ev1, true)
dur     = 10000;                        % Simulation length (ms)
ACh     = 1;                            % Learning toggle (Learning on, retrieval off)
k       = 2e-6;                         % Learning rate % original 2e-5; 
w_init 	= zeros(N,N);                   % Initial synaptic weight matrix (No associations encoded)
w       = cell(Agents,1);             % Memory to store output weight matrices
exIn    = PN;  
PNnotmodulation=0;
% External input generated by feature perception

for a   = 1 : Agents
    if PNnotmodulation
        exIn(a)=1.05;
    end
    
    [w{a},~]            = connectionist(dur,ACh,k,w_init,image,exIn(a));
    %w{a}=w{a}*normrnd(1,0.01);
    %exIn(a)=0.5;
    
end
clear a w_init

%%
%  Next, compute sensory evidence for the IAT based on the output
%  of the self-image network

Agents   
stim    = [1 2 3 4];                    % IAT stimuli (ev1 / ev2 / true / false)
ACh     = 0;                            % Learning toggle (Learning off, retrieval on)
dur     = 1000;                         % Simulation length (ms)
Con 	= nan(Agents,length(stim));     % Assign some memory for the output
Inc     = nan(Agents,length(stim));
r_total = nan(Agents,length(stim),2);
exIn=1.3;

for a   = 1 : Agents                  % Cycle through each agent...
    
    for s   = 1 : length(stim)          % ...and each IAT stimulus category
        
        % Compute the output firing rate of the network in response to that
        % stimulus category
        [~,r_out]       = connectionist(dur,ACh,k,w{a},stim(s),exIn);
        
        % Then store the total input to the motor response populations of
        % the drift diffusion network for congruent and incongruent blocks
        % separately
        Con(a,s)     = r_out(1,end)+r_out(3,end)-r_out(2,end)-r_out(4,end);
        Inc(a,s)     = r_out(1,end)+r_out(4,end)-r_out(2,end)-r_out(3,end);
        r_total(a,s,1)	= sum(r_out(:,end));
        %clear r_out
        
    end
    
end
Con  = (abs(Con) - exIn + 0.05) .* repmat([1 -1 1 -1],Agents,1);
Inc  = (abs(Inc) - exIn + 0.05) .* repmat([1 -1 -1 1],Agents,1);
clear a s




function[Weights,OutputRates] = connectionist(SimLength,ACh,k,Weights,Stimulus,Stim_Strength)
%% Function to simulate a simple connectionist network
%
%  Inputs:
%  SimLength        = length of simulation (ms)
%  ACh              = abstract neuromodulator permitting plasticity (1) vs
%                     recurrent currents (0)
%  k                = learning rate
%  Weights          = initial synaptic weight matrix
%  Stimulus         = indices of externally stimulated neurons
%  Stim_Strength    = strength of external excitation


%% Set parameters
Tau_r           = 10;       % Neuron time constant (ms)
r_max           = 10;       % Maximum firing rate (Hz)
r_t             = 5;        % Firing rate threshold (Hz)
wlim            = 0.075;    % Weight limit (au)


%% Establish connectivity and external input
N               = size(Weights,1);
if size(Weights,2) ~=N
    error('Weight matrix must be square...')
end
Connectivity    = ones(N,N);
for i           = 1 : N
    Connectivity(i,i)   = 0;
end
clear i
ExternalInput   = zeros(N,1);
ExternalInput(Stimulus) = Stim_Strength;


%% Assign memory for the output
OutputRates     = zeros(N,SimLength);


%% Run network dynamics
for t           = 1 : SimLength
    
    % Compute recurrent excitation
    RecurrentInput      = (1 - ACh) * sum(repmat(OutputRates(:,t),1,N) .* Connectivity .* Weights)';
    
    % Compute the value of the transfer function
    Transfer            = ExternalInput + RecurrentInput; clear RecurrentInput
    Transfer(Transfer > r_max)  = r_max;
    
    % Use this to calculate the output firing rate of the neuron
    OutputRates(:,t+1)  = OutputRates(:,t) + (1/Tau_r) * (-OutputRates(:,t) + Transfer); 
    
    clear Transfer;
    
    % Then update the synaptic weights based on input and output rates with
    % a Hebbian learning rule
    ri_rj               = repmat(OutputRates(:,t+1),1,N) .* repmat(OutputRates(:,t+1)',N,1) .* Connectivity;
    Weights             = Weights + ACh* k .* ri_rj; 
    clear ri_rj;
    c(t,1)=Weights(1,3);
    Weights(Weights>wlim)       = wlim;
    
    
end
%plot(c)

end
